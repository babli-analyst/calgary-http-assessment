Transcript
Speaker
Hello, my name is Pabli Pumari and in this video I will walk you through my approach to analyzing the Caligari HTTP dataset and covering data loading, cleaning and key insights. So I'm going to share my screen so I can
So you my Jupyter notebook. Here I have completed everything. So I downloaded the compressed log file using FileZilla due to browser HTTP limitation. After decompressing, I read the file line by line in Python using regular expression to parse each log entry into a structured field.
And then I printed the first few lines to understand the data structure which informed my parsing strategy. And using rejects, I extracted fields like host, timestamp, request, testers, code, and bytes. I converted timestamp to datetime object.
DateTime object handled missing values and extracted additional information like file extension for analysis. I used df.info and df.isnull.sum to verify data types and check for null values, ensuring the dataset was clean for analysis.
Check for unique values here So checking the unique values for all the column so I can see which kind of data In this data set so after that after cleaning part So then I Replace the
dash with unavailable for column RFC931 and in this column I get this dash also so I have replaced this with non authenticated and then I check for unique and now we have this unavailable
After that I checked for unique records for each and every column. After the data cleaning I started with the question part and in this question I calculated the total number of
HTTP request by determining the length of the data frame and then a unique host used an unique on the host column to find the number of distinct host and in this question I grouped data by date and counted unique file names to absorb daily access pattern
uh let's come to the discussion for number question in this question filter entries with a 404 status code to count not found errors and in this column we don't have the record of 404 and i also verify this from here
in this column you don't have any the course of 404 so this is the right answer here yeah this one is the right answer and I also checked here also so question number five total 15 file names with
top 15 filenames with 404 responses. So here we also don't get any records. Here I identified the most frequently requested missing resources by counting filenames in 404 errors. After that
Six number question we have to calculate the top 15 file extension with 404 responses Analyzed file extension in 404 errors to determine which file type so we are commonly missing Let's talk about question number seven
uh total bandwidth transferred per day for july so for that first of all we i checked for our data type and how many data uh here and the column names for that i have used df.info and uh here uh question number
Seven. Filter July 1995 data and some bytes per day to calculate daily bandwidth uses. And for question number eight, top... Sorry. Yeah. Yeah, question number...
Hardly request distribution. So for that I have used two date time to convert the data type of TimeStamp and then I checked, I calculated the hour and I calculated the value count of hour.
yeah like every hour so and then i converted into dict so here we will get records and then top 10 most requested file names here i'm counting the value i'm using the value counts to count the records uh respective uh early uh this uh
file name so and then i sort values in ascending equal to false which is considered as a descending order and here i'm using head 10 so it will give me the 10 records of that and then i'm using list so the data the record the output will return in list format
and this is the last question our total sorry HTTP response code distribution so in this I am using value count so this will count the value of this column so and I'm using the dict so the output will give me in a dictionary format which is
according to questions which I get in assessment. So and now comes into challenge phase. Parsing the Apache log format was challenging due to inconsistent and anomalies. Crafting a robust regular expression required careful consideration.
to accurately extract all the necessary field and now I want to come into conclusion part and I want to say like this analysis provided insights into server usage patterns, error, occurrence and bandwidth consumption in July and July
1995 and it highlighted the importance of thorough data cleaning and efficient processing technique so this was i have done this much data cleaning and the question which i have given to complete so i have done that so thank you so much
Okay.